{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijju-rasala/Bits-Capstone/blob/main/Capstone_Project_Helmet_Violation_Detection_Group_latest2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6986e06"
      },
      "source": [
        "# Helmet Violation Detection from Indian CCTV Video\n",
        "\n",
        "**Problem statement:**\n",
        "Detect and flag two-wheeler helmet violations (helmetless riding) from traffic camera frames in Indian cities in real-time.\n",
        "\n",
        "**Description:**\n",
        "Create a computer vision system using YOLOv8 and object tracking to detect two-wheeler riders and classify helmet usage. Optionally perform license plate OCR for enforcement.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9537eb41"
      },
      "source": [
        "## 1. Setup and Data Loading\n",
        "\n",
        "This section covers mounting Google Drive to access the dataset and extracting the data from the zip file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e39985cb"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "Mount your Google Drive to access files stored there persistently. This is necessary because files uploaded directly to Colab's temporary storage (`/content/sample_data/`) are deleted when the runtime ends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712d5c5f",
        "outputId": "51cc38ba-c859-4424-f0eb-c088bd2557f2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive at /content/drive\n",
        "# This will prompt you to authorize Colab to access your Google Drive files.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Note: Ensure your Google Drive path does not contain spaces if mounting to a custom directory.\n",
        "# Mounting to the default '/content/drive' is generally recommended."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78a1211f"
      },
      "source": [
        "### Extract Dataset from Zip File\n",
        "\n",
        "Extract the contents of the dataset zip file stored in your Google Drive to a designated folder within your Drive. This ensures the unzipped data is also persistent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22998f32",
        "outputId": "9642bcee-1de8-463c-ff36-b27e9aef5da9"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to your zip file in Google Drive\n",
        "# Ensure this path correctly points to where you uploaded archive2.zip in your Drive.\n",
        "zip_file_path = '/content/drive/My Drive/capstone_helmet_detection/archive2.zip'\n",
        "\n",
        "# Define the directory within Google Drive where the contents will be extracted\n",
        "# This folder will be created if it doesn't exist.\n",
        "extraction_path = '/content/drive/My Drive/capstone_helmet_detection/unzipped_archive'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all contents to the specified extraction_path\n",
        "        zip_ref.extractall(extraction_path)\n",
        "    print(f\"Successfully extracted {zip_file_path} to {extraction_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {zip_file_path}\")\n",
        "    print(\"Please ensure the zip file is in the correct location in your Google Drive.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file at {zip_file_path} is not a valid zip file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Define the base directory for the extracted data based on the likely internal structure of the zip\n",
        "# If the zip extracts into a subfolder like 'archive' within unzipped_archive, adjust this path.\n",
        "# Based on previous successful runs, the path seems to be unzipped_archive/archive/train and unzipped_archive/archive/val\n",
        "extracted_data_base_dir = os.path.join(extraction_path, 'archive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted /content/drive/My Drive/capstone_helmet_detection/archive2.zip to /content/drive/My Drive/capstone_helmet_detection/unzipped_archive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c58e5bbb"
      },
      "source": [
        "## 2. Data Exploration and Analysis\n",
        "\n",
        "This section explores the dataset's characteristics, including file counts, image sizes, class distribution, and bounding box quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79516bcd"
      },
      "source": [
        "### Analyze Class Distribution\n",
        "\n",
        "Analyze the distribution of object classes within the dataset's annotations. This helps identify class imbalance, which might require specific handling during model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bb33113",
        "outputId": "f7493e32-5c16-49f9-a5ab-12fba9c43e60"
      },
      "source": [
        "import os\n",
        "\n",
        "annotations_dir = os.path.join(extracted_data_base_dir, 'train', 'labels')\n",
        "\n",
        "# Define the directory containing the annotation files\n",
        "# Uses the annotations_dir defined previously\n",
        "# annotations_dir = os.path.join(extracted_data_base_dir, 'train', 'labels')\n",
        "\n",
        "# Dictionary to store class counts\n",
        "class_counts = {}\n",
        "\n",
        "# Iterate through each annotation file\n",
        "if os.path.exists(annotations_dir):\n",
        "    for filename in os.listdir(annotations_dir):\n",
        "        # Process only .txt files (assuming YOLO format annotations)\n",
        "        if filename.lower().endswith('.txt'):\n",
        "            annotation_path = os.path.join(annotations_dir, filename)\n",
        "            try:\n",
        "                # Read each line in the annotation file\n",
        "                with open(annotation_path, 'r') as f:\n",
        "                    for line in f:\n",
        "                        try:\n",
        "                            # The first value in each line is the class_id (YOLO format)\n",
        "                            class_id = int(line.split()[0])\n",
        "                            # Increment the count for this class_id\n",
        "                            if class_id in class_counts:\n",
        "                                class_counts[class_id] += 1\n",
        "                            else:\n",
        "                                class_counts[class_id] = 1\n",
        "                        except (ValueError, IndexError):\n",
        "                            # Handle lines that might not be in the expected format\n",
        "                            print(f\"Skipping malformed line in {filename}: {line.strip()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read annotation file {filename}: {e}\")\n",
        "else:\n",
        "    print(f\"Error: Annotations directory not found at {annotations_dir}\")\n",
        "\n",
        "\n",
        "# Print the class distribution\n",
        "print(\"Class distribution across all annotations:\")\n",
        "# Sort by class ID for consistent output\n",
        "for class_id in sorted(class_counts.keys()):\n",
        "    print(f\"Class {class_id}: {class_counts[class_id]}\")\n",
        "\n",
        "# Store class counts for later use if needed\n",
        "# class_counts_dict = class_counts"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution across all annotations:\n",
            "Class 0: 1327\n",
            "Class 1: 791\n",
            "Class 2: 1787\n",
            "Class 3: 293\n",
            "Class 4: 1750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b067635"
      },
      "source": [
        "### Define Class Names\n",
        "\n",
        "Based on manual inspection or dataset documentation, define the mapping of class IDs to human-readable names. This is crucial for interpreting results and configuring YOLOv8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fed8eaea",
        "outputId": "cc10d05f-852b-42b1-e4f9-f51d204fd7bd"
      },
      "source": [
        "# Define the class names based on the mapping you provided:\n",
        "# 0 → Vehicle number\n",
        "# 1 → Rider without helmet\n",
        "# 2 → Rider with valid helmet\n",
        "# 3 → Rider with invalid helmet\n",
        "# 4 → Motorbike with a person\n",
        "\n",
        "class_names = ['Vehicle number', 'Rider without helmet', 'Rider with valid helmet', 'Rider with invalid helmet', 'Motorbike with a person']\n",
        "num_classes = len(class_names) # Number of classes\n",
        "\n",
        "print(\"Defined class names:\")\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"Class {i}: {name}\")\n",
        "\n",
        "# Store class names and number of classes for later use\n",
        "# class_names_list = class_names\n",
        "# num_classes_int = num_classes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined class names:\n",
            "Class 0: Vehicle number\n",
            "Class 1: Rider without helmet\n",
            "Class 2: Rider with valid helmet\n",
            "Class 3: Rider with invalid helmet\n",
            "Class 4: Motorbike with a person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b94e79e"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Data Augmentation\n",
        "\n",
        "This section focuses on applying data augmentation techniques to the dataset to increase its size and variability, which can help improve model performance, especially for detecting small objects and addressing class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8cfb8a"
      },
      "source": [
        "### Install Augmentation Library\n",
        "\n",
        "Install a suitable data augmentation library for object detection (e.g., Albumentations) and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d407f28",
        "outputId": "3e759c7a-842c-4c9d-d923-96e883452b8b"
      },
      "source": [
        "# Install Albumentations and opencv-python-headless\n",
        "# opencv-python-headless is used by Albumentations for image processing\n",
        "%pip install albumentations opencv-python-headless\n",
        "\n",
        "# Note: You might need to restart the runtime after installation if prompted."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.9)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.0.11)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________________________________________________\n"
      ],
      "metadata": {
        "id": "qfjHxDfimgjs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJvmRjIuf5Gn",
        "outputId": "30cc2ef4-9f53-4123-a558-8957bbc38ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined data.yaml path: /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "136d2a78",
        "outputId": "fc5c3341-d1d2-45ee-b5a8-8f1a22c7d5e8"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a YOLOv8 model. Choose a smaller model like 'yolov8n.pt' (nano) or 'yolov8s.pt' (small)\n",
        "# for faster training and lower resource usage compared to 'yolov8m.pt' (medium) or larger models.\n",
        "# 'yolov8s.pt' offers a good balance between speed and performance.\n",
        "model = YOLO('yolov8n.pt') # Using the small version of YOLOv8\n",
        "\n",
        "print(f\"YOLO model loaded: {model.model.yaml['nc']} classes, {model.model.yaml['ch']} channels\")\n",
        "\n",
        "# Assuming data_yaml_path_final is defined from a previous cell pointing to your data.yaml\n",
        "# You can optionally specify project and name for the training run to organize results\n",
        "results = model.train(data=data_yaml_path_final, epochs=10, imgsz=416, batch=16, project='helmet_detection_training', name='yolov8s_augmented_data') # Reduced epochs for faster testing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO model loaded: 80 classes, 3 channels\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8s_augmented_data4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=helmet_detection_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/helmet_detection_training/yolov8s_augmented_data4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.2 ms, read: 35.5±9.2 MB/s, size: 67.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 800/800 1.1Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.6±0.2 ms, read: 25.0±9.1 MB/s, size: 62.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 32.5Kit/s 0.0s\n",
            "Plotting labels to /content/helmet_detection_training/yolov8s_augmented_data4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/helmet_detection_training/yolov8s_augmented_data4\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10       2.7G      2.108      2.968       1.64        109        416: 100% ━━━━━━━━━━━━ 50/50 4.2it/s 12.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.8it/s 2.8s\n",
            "                   all        142       1064      0.739      0.131      0.166     0.0799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10      2.72G      1.903      1.877      1.464        104        416: 100% ━━━━━━━━━━━━ 50/50 5.5it/s 9.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.7it/s 1.3s\n",
            "                   all        142       1064       0.53      0.254       0.29      0.111\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10      2.72G      1.871      1.748      1.452        127        416: 100% ━━━━━━━━━━━━ 50/50 4.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.7it/s 1.3s\n",
            "                   all        142       1064      0.449      0.409      0.397       0.17\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10      2.73G      1.842      1.658      1.432        124        416: 100% ━━━━━━━━━━━━ 50/50 4.9it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.9it/s 1.3s\n",
            "                   all        142       1064      0.473      0.466      0.438      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10      2.75G      1.822      1.566      1.408        100        416: 100% ━━━━━━━━━━━━ 50/50 5.0it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.6it/s 1.4s\n",
            "                   all        142       1064      0.502      0.427      0.466      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10      2.75G      1.787      1.507      1.386         91        416: 100% ━━━━━━━━━━━━ 50/50 5.3it/s 9.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.4it/s 2.1s\n",
            "                   all        142       1064      0.532      0.459      0.508      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10      2.75G      1.744      1.445      1.383        127        416: 100% ━━━━━━━━━━━━ 50/50 6.0it/s 8.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.5it/s 1.4s\n",
            "                   all        142       1064      0.729      0.458      0.518      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10      2.75G      1.732      1.396      1.346        125        416: 100% ━━━━━━━━━━━━ 50/50 5.1it/s 9.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.2it/s 1.2s\n",
            "                   all        142       1064      0.741       0.51      0.556      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10      2.77G      1.684      1.326      1.331        104        416: 100% ━━━━━━━━━━━━ 50/50 4.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.7it/s 1.3s\n",
            "                   all        142       1064       0.52       0.53      0.553      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10      2.78G      1.677       1.29      1.328        118        416: 100% ━━━━━━━━━━━━ 50/50 4.9it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.2it/s 1.5s\n",
            "                   all        142       1064       0.72      0.525      0.565      0.255\n",
            "\n",
            "10 epochs completed in 0.035 hours.\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8s_augmented_data4/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8s_augmented_data4/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/helmet_detection_training/yolov8s_augmented_data4/weights/best.pt...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.2it/s 2.3s\n",
            "                   all        142       1064      0.711      0.528      0.565      0.255\n",
            "                     0        117        258      0.606      0.539      0.554      0.194\n",
            "                     1         59        179      0.612      0.475      0.562      0.212\n",
            "                     2        114        274      0.596      0.712      0.688      0.311\n",
            "                     3         28         53          1          0      0.122     0.0581\n",
            "                     4        125        300      0.741      0.913      0.897      0.499\n",
            "Speed: 0.1ms preprocess, 1.8ms inference, 0.0ms loss, 3.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/helmet_detection_training/yolov8s_augmented_data4\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34340a04",
        "outputId": "ee13f414-7a16-4ada-af47-106ed3cad850"
      },
      "source": [
        "%pip install ultralytics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.203)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SINf55JwcavG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21347cf"
      },
      "source": [
        "# Task -- 06/10 - 10 AM\n",
        "Train the YOLOv8 model without augmentation using the configuration file \"/content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml\" and evaluate its performance. Then, train the model with default augmentation using the same configuration and evaluate its performance. Finally, collect and compare the results from both training runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33fd3858"
      },
      "source": [
        "## Train without augmentation\n",
        "\n",
        "### Subtask:\n",
        "Train the YOLOv8 model with the specified parameters but with `augment=False` to disable all augmentations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b54d076b"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the YOLOv8 model with the specified parameters and disabled augmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "81b25605",
        "outputId": "12174d7b-6c54-42b2-b11d-7ad5b7a11b5e"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a YOLOv8 model. Using 'yolov8n.pt' as it's smaller and faster for demonstration.\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Assuming data_yaml_path_final is defined from a previous cell pointing to your data.yaml\n",
        "# Assuming class_names is defined from a previous cell\n",
        "\n",
        "# Train the model with specified parameters and augmentation disabled\n",
        "results_no_aug = model.train(\n",
        "    data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "    epochs=10,                  # Number of training epochs\n",
        "    imgsz=640,                  # Image size for training\n",
        "    batch=16,                   # Batch size\n",
        "    augment=False,              # Disable data augmentation\n",
        "    project='helmet_detection_training', # Project name to group runs\n",
        "    name='yolov8n_no_augmentation'     # Name for this specific run\n",
        ")\n",
        "\n",
        "print(\"Training without augmentation completed.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ultralytics'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3885179489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load a YOLOv8 model. Using 'yolov8n.pt' as it's smaller and faster for demonstration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov8n.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6e41af4"
      },
      "source": [
        "**Reasoning**:\n",
        "The first training run without augmentation is complete. The next step is to train the model with default augmentation enabled.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a24a196",
        "outputId": "13915809-b1cf-415e-dd2a-af21cb6d4734"
      },
      "source": [
        "# Assuming data_yaml_path_final is defined from a previous cell pointing to your data.yaml\n",
        "# Assuming class_names is defined from a previous cell\n",
        "# The YOLO model is already loaded in the previous step\n",
        "\n",
        "# Train the model with specified parameters and default augmentation enabled\n",
        "results_with_aug = model.train(\n",
        "    data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "    epochs=10,                  # Number of training epochs\n",
        "    imgsz=640,                  # Image size for training\n",
        "    batch=16,                   # Batch size\n",
        "    augment=True,               # Enable default data augmentation\n",
        "    project='helmet_detection_training', # Project name to group runs\n",
        "    name='yolov8n_with_augmentation'      # Name for this specific run\n",
        ")\n",
        "\n",
        "print(\"Training with augmentation completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_with_augmentation, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=helmet_detection_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/helmet_detection_training/yolov8n_with_augmentation, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.7±0.2 ms, read: 40.6±10.7 MB/s, size: 67.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 800/800 738.6Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 3.0±1.1 ms, read: 5.9±2.1 MB/s, size: 62.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 101.9Kit/s 0.0s\n",
            "Plotting labels to /content/helmet_detection_training/yolov8n_with_augmentation/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/helmet_detection_training/yolov8n_with_augmentation\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10      1.43G       1.68        1.3      1.325        109        416: 100% ━━━━━━━━━━━━ 50/50 4.1it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.4it/s 2.1s\n",
            "                   all        142       1064       0.71      0.513      0.546      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10       1.6G      1.681      1.326      1.336        104        416: 100% ━━━━━━━━━━━━ 50/50 5.7it/s 8.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.9it/s 1.7s\n",
            "                   all        142       1064      0.485      0.566      0.526       0.22\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10       1.6G      1.694       1.34      1.336        127        416: 100% ━━━━━━━━━━━━ 50/50 5.2it/s 9.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.2it/s 1.5s\n",
            "                   all        142       1064      0.511      0.491      0.495      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10      1.62G      1.696      1.339      1.346        124        416: 100% ━━━━━━━━━━━━ 50/50 4.7it/s 10.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.7it/s 1.4s\n",
            "                   all        142       1064       0.54      0.546      0.533      0.227\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10      1.62G      1.685      1.283      1.331        100        416: 100% ━━━━━━━━━━━━ 50/50 4.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.0it/s 1.3s\n",
            "                   all        142       1064      0.702      0.504      0.519      0.222\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10      1.64G      1.659      1.254       1.32         91        416: 100% ━━━━━━━━━━━━ 50/50 4.8it/s 10.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.4it/s 1.5s\n",
            "                   all        142       1064      0.526      0.551      0.546      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10      1.65G      1.636      1.217      1.315        127        416: 100% ━━━━━━━━━━━━ 50/50 4.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.3it/s 1.5s\n",
            "                   all        142       1064      0.545      0.526      0.562       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10      1.65G       1.64      1.187      1.293        125        416: 100% ━━━━━━━━━━━━ 50/50 5.3it/s 9.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.6it/s 1.9s\n",
            "                   all        142       1064      0.552      0.585      0.586      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10      1.65G        1.6      1.127      1.286        104        416: 100% ━━━━━━━━━━━━ 50/50 5.5it/s 9.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.5it/s 1.4s\n",
            "                   all        142       1064       0.58      0.567      0.594      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10      1.67G      1.603      1.127      1.288        118        416: 100% ━━━━━━━━━━━━ 50/50 4.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.3it/s 1.5s\n",
            "                   all        142       1064      0.591      0.576      0.596      0.274\n",
            "\n",
            "10 epochs completed in 0.035 hours.\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8n_with_augmentation/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8n_with_augmentation/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/helmet_detection_training/yolov8n_with_augmentation/weights/best.pt...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.1it/s 4.5s\n",
            "                   all        142       1064       0.74      0.535      0.581      0.272\n",
            "                     0        117        258      0.718      0.547      0.594      0.217\n",
            "                     1         59        179      0.572      0.537      0.582      0.228\n",
            "                     2        114        274       0.65      0.686      0.683      0.329\n",
            "                     3         28         53          1          0      0.147     0.0754\n",
            "                     4        125        300      0.762      0.907        0.9      0.513\n",
            "Speed: 0.3ms preprocess, 13.8ms inference, 0.0ms loss, 4.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/helmet_detection_training/yolov8n_with_augmentation\u001b[0m\n",
            "Training with augmentation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17f9158"
      },
      "source": [
        "### Evaluate model with default augmentation\n",
        "\n",
        "**Subtask:** Evaluate the performance of the model trained with default augmentations using `model.val()` to get detailed metrics, including per-class results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8113540a",
        "outputId": "78f254f7-7bae-4d68-8a0c-23aa0619c651"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Define the path to the best weights of the model trained with default augmentation\n",
        "# This path is based on the 'project' and 'name' parameters used during training in cell 5a24a196\n",
        "# Ensure this path matches where your weights were saved\n",
        "weights_path_with_aug = '/content/helmet_detection_training/yolov8n_with_augmentation/weights/best.pt'\n",
        "\n",
        "# Check if the weights file exists\n",
        "if os.path.exists(weights_path_with_aug):\n",
        "    # Load the trained model\n",
        "    model_with_aug = YOLO(weights_path_with_aug)\n",
        "    print(f\"Model trained with default augmentation loaded successfully from {weights_path_with_aug}\")\n",
        "else:\n",
        "    print(f\"Error: Trained weights not found at {weights_path_with_aug}\")\n",
        "    print(\"Please verify the path to the saved weights.\")\n",
        "    model_with_aug = None # Set to None if loading fails"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained with default augmentation loaded successfully from /content/helmet_detection_training/yolov8n_with_augmentation/weights/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e936df8c"
      },
      "source": [
        "### Evaluate model without augmentation\n",
        "\n",
        "**Subtask:** Load the model trained without augmentation and evaluate its performance using `model.val()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae8e89fd",
        "outputId": "d1668fa2-b85c-49f7-9ab6-dcb373a12f1d"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Define the path to the best weights of the model trained without augmentation\n",
        "# This path is based on the 'project' and 'name' parameters used during that training run\n",
        "# Ensure this path matches where your weights were saved\n",
        "weights_path_no_aug = '/content/helmet_detection_training/yolov8n_no_augmentation/weights/best.pt'\n",
        "\n",
        "# Check if the weights file exists\n",
        "if os.path.exists(weights_path_no_aug):\n",
        "    # Load the trained model\n",
        "    model_no_aug = YOLO(weights_path_no_aug)\n",
        "    print(f\"Model trained without augmentation loaded successfully from {weights_path_no_aug}\")\n",
        "\n",
        "    # Evaluate the model trained without augmentation\n",
        "    print(\"Evaluating model trained without augmentation...\")\n",
        "    results_val_no_aug = model_no_aug.val(\n",
        "        data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "        imgsz=640,                  # Image size for validation (should match training size)\n",
        "        batch=16,                   # Batch size for validation\n",
        "    )\n",
        "    print(\"Evaluation without augmentation completed.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Trained weights not found at {weights_path_no_aug}\")\n",
        "    print(\"Please verify the path to the saved weights and ensure the training without augmentation was completed successfully.\")\n",
        "    results_val_no_aug = None # Set to None if loading fails"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained without augmentation loaded successfully from /content/helmet_detection_training/yolov8n_no_augmentation/weights/best.pt\n",
            "Evaluating model trained without augmentation...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 26.5±15.7 MB/s, size: 69.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 267.6Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.1it/s 2.9s\n",
            "                   all        142       1064      0.707      0.529      0.564      0.255\n",
            "                     0        117        258      0.598      0.535      0.546      0.192\n",
            "                     1         59        179      0.602       0.48      0.562      0.211\n",
            "                     2        114        274      0.598      0.715      0.692      0.311\n",
            "                     3         28         53          1          0      0.123     0.0581\n",
            "                     4        125        300      0.737      0.914      0.897      0.501\n",
            "Speed: 2.0ms preprocess, 2.9ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val3\u001b[0m\n",
            "Evaluation without augmentation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18e84396",
        "outputId": "f025cc64-b241-4e74-b255-f371d4fa583c"
      },
      "source": [
        "# Assuming the model trained with default augmentation is available as 'model_with_aug'\n",
        "# and the data.yaml path is available as 'data_yaml_path_final'\n",
        "\n",
        "print(\"Evaluating model trained with default augmentation...\")\n",
        "\n",
        "# Run validation on the trained model\n",
        "results_val_with_aug = model_with_aug.val(\n",
        "    data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "    imgsz=640,                  # Image size for validation (should match training size)\n",
        "    batch=16,                   # Batch size for validation\n",
        ")\n",
        "\n",
        "print(\"Evaluation with default augmentation completed.\")\n",
        "\n",
        "# Store the validation results for comparison\n",
        "# results_val_with_aug_metrics = results_val_with_aug.results_dict # This gives overall metrics\n",
        "# To get per-class metrics, we might need to access other attributes or parse the output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model trained with default augmentation...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 42.7±8.6 MB/s, size: 68.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 155.7Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.1it/s 2.9s\n",
            "                   all        142       1064      0.585      0.566      0.594      0.275\n",
            "                     0        117        258      0.714      0.566      0.596       0.21\n",
            "                     1         59        179      0.627      0.581      0.612      0.238\n",
            "                     2        114        274      0.623       0.73      0.702      0.333\n",
            "                     3         28         53      0.178     0.0189      0.155       0.08\n",
            "                     4        125        300      0.783      0.933      0.904      0.514\n",
            "Speed: 2.0ms preprocess, 3.7ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n",
            "Evaluation with default augmentation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f9b503b"
      },
      "source": [
        "## Collect and compare results\n",
        "\n",
        "### Subtask:\n",
        "Collect and compare the results from both training runs (without augmentation and with default augmentation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79242cd4"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract and compare the performance metrics from the results objects of both training runs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "d1471dbe",
        "outputId": "f3b7f191-692b-4219-9af6-b847eb207ce1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Access the overall performance metrics from the validation results\n",
        "# These are available in the results_dict attribute of the validation results object\n",
        "metrics_no_aug = results_val_no_aug.results_dict\n",
        "map50_no_aug = metrics_no_aug['metrics/mAP50(B)']\n",
        "map50_95_no_aug = metrics_no_aug['metrics/mAP50-95(B)']\n",
        "\n",
        "metrics_with_aug = results_val_with_aug.results_dict\n",
        "map50_with_aug = metrics_with_aug['metrics/mAP50(B)']\n",
        "map50_95_with_aug = metrics_with_aug['metrics/mAP50-95(B)']\n",
        "\n",
        "# Access the overall performance metrics from the custom augmentation (General) validation results\n",
        "metrics_custom_aug = results_val_custom_aug.results_dict\n",
        "map50_custom_aug = metrics_custom_aug['metrics/mAP50(B)']\n",
        "map50_95_custom_aug = metrics_custom_aug['metrics/mAP50-95(B)']\n",
        "\n",
        "# Access the overall performance metrics from the blur-focused custom augmentation validation results\n",
        "metrics_custom_aug_blur = results_val_custom_aug_blur.results_dict\n",
        "map50_custom_aug_blur = metrics_custom_aug_blur['metrics/mAP50(B)']\n",
        "map50_95_custom_aug_blur = metrics_custom_aug_blur['metrics/mAP50-95(B)']\n",
        "\n",
        "# Access the overall performance metrics from the CutMix attempt custom augmentation validation results\n",
        "metrics_cutmix_attempt = results_val_cutmix_attempt.results_dict\n",
        "map50_cutmix_attempt = metrics_cutmix_attempt['metrics/mAP50(B)']\n",
        "map50_95_cutmix_attempt = metrics_cutmix_attempt['metrics/mAP50-95(B)']\n",
        "\n",
        "\n",
        "# Create a dictionary to store the overall comparison\n",
        "comparison_data_overall = {\n",
        "    'Metric': ['mAP@0.5', 'mAP@0.5:0.95'],\n",
        "    'Without Augmentation': [map50_no_aug, map50_95_no_aug],\n",
        "    'With Default Augmentation': [map50_with_aug, map50_95_with_aug],\n",
        "    'With Custom Augmentation (General)': [map50_custom_aug, map50_95_custom_aug],\n",
        "    'With Custom Augmentation (Blur Focus)': [map50_custom_aug_blur, map50_95_custom_aug_blur],\n",
        "    'With Custom Augmentation (CutMix Attempt)': [map50_cutmix_attempt, map50_95_cutmix_attempt]\n",
        "}\n",
        "\n",
        "# Create a pandas DataFrame for a clear comparison table (overall metrics)\n",
        "df_comparison_overall = pd.DataFrame(comparison_data_overall)\n",
        "\n",
        "# Print the overall comparison table\n",
        "print(\"Overall Performance Comparison:\")\n",
        "display(df_comparison_overall)\n",
        "\n",
        "# Access individual class mAP@0.5 from the validation results\n",
        "# These are typically available in the box.map50 attribute as a numpy array\n",
        "class_map50_no_aug_values = results_val_no_aug.box.map50 # NumPy array of mAP@0.5 for each class\n",
        "class_map50_with_aug_values = results_val_with_aug.box.map50 # NumPy array of mAP@0.5 for each class\n",
        "class_map50_custom_aug_values = results_val_custom_aug.box.map50 # NumPy array of mAP@0.5 for each class\n",
        "class_map50_custom_aug_blur_values = results_val_custom_aug_blur.box.map50 # NumPy array of mAP@0.5 for each class\n",
        "class_map50_cutmix_attempt_values = results_val_cutmix_attempt.box.map50 # NumPy array of mAP@0.5 for each class\n",
        "\n",
        "\n",
        "# Create a pandas DataFrame for individual class mAP@0.5 comparison\n",
        "df_class_comparison_map50 = pd.DataFrame({\n",
        "    'Class': class_names, # Assuming class_names list is available\n",
        "    'mAP@0.5 (No Aug)': class_map50_no_aug_values,\n",
        "    'mAP@0.5 (With Default Aug)': class_map50_with_aug_values,\n",
        "    'mAP@0.5 (With Custom Aug - General)': class_map50_custom_aug_values,\n",
        "    'mAP@0.5 (With Custom Aug - Blur Focus)': class_map50_custom_aug_blur_values,\n",
        "    'mAP@0.5 (With Custom Aug - CutMix Attempt)': class_map50_cutmix_attempt_values\n",
        "})\n",
        "\n",
        "# Print the individual class mAP@0.5 comparison table\n",
        "print(\"\\nIndividual Class mAP@0.5 Comparison:\")\n",
        "display(df_class_comparison_map50)\n",
        "\n",
        "# Note: You can similarly access precision and recall per class if needed\n",
        "# results_val_no_aug.box.p # Precision per class\n",
        "# results_val_no_aug.box.r # Recall per class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Performance Comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Metric  Without Augmentation  With Default Augmentation  \\\n",
              "0       mAP@0.5              0.563963                   0.593813   \n",
              "1  mAP@0.5:0.95              0.254623                   0.275126   \n",
              "\n",
              "   With Custom Augmentation (General)  With Custom Augmentation (Blur Focus)  \n",
              "0                            0.551806                               0.551806  \n",
              "1                            0.251758                               0.251758  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a92f0011-7bb5-4608-9fb5-c8a22fb1f395\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Without Augmentation</th>\n",
              "      <th>With Default Augmentation</th>\n",
              "      <th>With Custom Augmentation (General)</th>\n",
              "      <th>With Custom Augmentation (Blur Focus)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mAP@0.5</td>\n",
              "      <td>0.563963</td>\n",
              "      <td>0.593813</td>\n",
              "      <td>0.551806</td>\n",
              "      <td>0.551806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mAP@0.5:0.95</td>\n",
              "      <td>0.254623</td>\n",
              "      <td>0.275126</td>\n",
              "      <td>0.251758</td>\n",
              "      <td>0.251758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a92f0011-7bb5-4608-9fb5-c8a22fb1f395')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a92f0011-7bb5-4608-9fb5-c8a22fb1f395 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a92f0011-7bb5-4608-9fb5-c8a22fb1f395');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0d3dd1d2-1160-4f58-a6d2-689d09c22e16\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d3dd1d2-1160-4f58-a6d2-689d09c22e16')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0d3dd1d2-1160-4f58-a6d2-689d09c22e16 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_25574c6f-b15a-4f9d-afa1-001a4b68aaef\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_comparison_overall')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_25574c6f-b15a-4f9d-afa1-001a4b68aaef button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_comparison_overall');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_comparison_overall",
              "summary": "{\n  \"name\": \"df_comparison_overall\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"mAP@0.5:0.95\",\n          \"mAP@0.5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Without Augmentation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2187359818765736,\n        \"min\": 0.25462349466473577,\n        \"max\": 0.5639628868135816,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.25462349466473577,\n          0.5639628868135816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"With Default Augmentation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22534596789465733,\n        \"min\": 0.2751259059316959,\n        \"max\": 0.5938132299544123,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2751259059316959,\n          0.5938132299544123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"With Custom Augmentation (General)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21216567681996973,\n        \"min\": 0.2517580367952939,\n        \"max\": 0.5518056144241621,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2517580367952939,\n          0.5518056144241621\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"With Custom Augmentation (Blur Focus)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21216567681996973,\n        \"min\": 0.2517580367952939,\n        \"max\": 0.5518056144241621,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2517580367952939,\n          0.5518056144241621\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Individual Class mAP@0.5 Comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                       Class  mAP@0.5 (No Aug)  mAP@0.5 (With Default Aug)  \\\n",
              "0             Vehicle number          0.563963                    0.593813   \n",
              "1       Rider without helmet          0.563963                    0.593813   \n",
              "2    Rider with valid helmet          0.563963                    0.593813   \n",
              "3  Rider with invalid helmet          0.563963                    0.593813   \n",
              "4    Motorbike with a person          0.563963                    0.593813   \n",
              "\n",
              "   mAP@0.5 (With Custom Aug - General)  mAP@0.5 (With Custom Aug - Blur Focus)  \n",
              "0                             0.551806                                0.551806  \n",
              "1                             0.551806                                0.551806  \n",
              "2                             0.551806                                0.551806  \n",
              "3                             0.551806                                0.551806  \n",
              "4                             0.551806                                0.551806  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-971b40a6-83cf-48fb-a189-4c2a98810839\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>mAP@0.5 (No Aug)</th>\n",
              "      <th>mAP@0.5 (With Default Aug)</th>\n",
              "      <th>mAP@0.5 (With Custom Aug - General)</th>\n",
              "      <th>mAP@0.5 (With Custom Aug - Blur Focus)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Vehicle number</td>\n",
              "      <td>0.563963</td>\n",
              "      <td>0.593813</td>\n",
              "      <td>0.551806</td>\n",
              "      <td>0.551806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rider without helmet</td>\n",
              "      <td>0.563963</td>\n",
              "      <td>0.593813</td>\n",
              "      <td>0.551806</td>\n",
              "      <td>0.551806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rider with valid helmet</td>\n",
              "      <td>0.563963</td>\n",
              "      <td>0.593813</td>\n",
              "      <td>0.551806</td>\n",
              "      <td>0.551806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rider with invalid helmet</td>\n",
              "      <td>0.563963</td>\n",
              "      <td>0.593813</td>\n",
              "      <td>0.551806</td>\n",
              "      <td>0.551806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Motorbike with a person</td>\n",
              "      <td>0.563963</td>\n",
              "      <td>0.593813</td>\n",
              "      <td>0.551806</td>\n",
              "      <td>0.551806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-971b40a6-83cf-48fb-a189-4c2a98810839')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-971b40a6-83cf-48fb-a189-4c2a98810839 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-971b40a6-83cf-48fb-a189-4c2a98810839');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-11e46ae6-61aa-402f-a00b-4425a17c8e91\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11e46ae6-61aa-402f-a00b-4425a17c8e91')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-11e46ae6-61aa-402f-a00b-4425a17c8e91 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5778e33a-fe49-4ee8-a323-974a5423bb34\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_class_comparison_map50')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5778e33a-fe49-4ee8-a323-974a5423bb34 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_class_comparison_map50');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_class_comparison_map50",
              "summary": "{\n  \"name\": \"df_class_comparison_map50\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Rider without helmet\",\n          \"Motorbike with a person\",\n          \"Rider with valid helmet\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP@0.5 (No Aug)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.5639628868135816,\n        \"max\": 0.5639628868135816,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5639628868135816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP@0.5 (With Default Aug)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.5938132299544123,\n        \"max\": 0.5938132299544123,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5938132299544123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP@0.5 (With Custom Aug - General)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.5518056144241621,\n        \"max\": 0.5518056144241621,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5518056144241621\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP@0.5 (With Custom Aug - Blur Focus)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.5518056144241621,\n        \"max\": 0.5518056144241621,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5518056144241621\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56c0a0ad",
        "outputId": "1ad527f0-6860-4fab-8d8c-ea1ac422ed22"
      },
      "source": [
        "# Print all keys in the metrics dictionary to identify the correct class-wise metric keys\n",
        "print(\"Keys available in metrics_no_aug dictionary:\")\n",
        "print(metrics_no_aug.keys())\n",
        "\n",
        "print(\"\\nKeys available in metrics_with_aug dictionary:\")\n",
        "print(metrics_with_aug.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys available in metrics_no_aug dictionary:\n",
            "dict_keys(['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness'])\n",
            "\n",
            "Keys available in metrics_with_aug dictionary:\n",
            "dict_keys(['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e515bdf"
      },
      "source": [
        "## Choosing Augmentations Based on Data Variations\n",
        "\n",
        "When selecting data augmentation techniques for this helmet violation detection project, it's important to consider the variations expected in real-world Indian CCTV traffic footage. Augmentations can help the model become more robust to these variations.\n",
        "\n",
        "Here are some common variations and relevant Albumentations transforms to simulate them:\n",
        "\n",
        "1.  **Varying Lighting Conditions:**\n",
        "    *   **What you might see:** Bright sunlight, shadows, dusk/dawn, headlights at night, glare.\n",
        "    *   **Relevant Augmentations:**\n",
        "        *   `A.RandomBrightnessContrast`: Adjusts the overall brightness and contrast.\n",
        "        *   `A.CLAHE`: Improves contrast in different regions.\n",
        "        *   `A.RandomGamma`: Changes the intensity of colors.\n",
        "        *   `A.HueSaturationValue`: Alters color tones, saturation, and brightness.\n",
        "\n",
        "2.  **Different Angles and Perspectives:**\n",
        "    *   **What you might see:** Riders and vehicles viewed from different angles (side, front, back, slightly elevated or lowered camera).\n",
        "    *   **Relevant Augmentations:**\n",
        "        *   `A.ShiftScaleRotate`: Applies random shifts, scaling, and rotations.\n",
        "        *   `A.Perspective`: Applies a random perspective transformation.\n",
        "\n",
        "3.  **Blurriness:**\n",
        "    *   **What you might see:** Motion blur, focus issues, camera shake.\n",
        "    *   **Relevant Augmentations:**\n",
        "        *   `A.Blur`, `A.MedianBlur`, `A.MotionBlur`.\n",
        "\n",
        "4.  **Occlusions and Partial Visibility:**\n",
        "    *   **What you might see:** Parts of the rider or vehicle being blocked.\n",
        "    *   **Relevant Augmentations:**\n",
        "        *   `A.CoarseDropout`: Randomly drops rectangular regions.\n",
        "\n",
        "5.  **Noise:**\n",
        "    *   **What you might see:** Electrical noise, compression artifacts.\n",
        "    *   **Relevant Augmentations:**\n",
        "        *   `A.GaussNoise`.\n",
        "\n",
        "By choosing augmentations that mimic these real-world conditions, we can help the model generalize better to unseen data. You can experiment with different combinations and parameters of these augmentations to find what works best for your specific dataset and target performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24f0f304"
      },
      "source": [
        "### Define custom augmentation pipeline\n",
        "\n",
        "**Subtask:** Create a custom data augmentation pipeline using Albumentations with specific hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf30fdd",
        "outputId": "08bfe031-c171-4528-b44b-d3e47e37425b"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a fresh YOLOv8 model for this training run\n",
        "# Using 'yolov8n.pt' as before\n",
        "model_with_custom_aug_blur = YOLO('yolov8n.pt')\n",
        "\n",
        "# Assuming data_yaml_path_final is defined from a previous cell pointing to your data.yaml\n",
        "# Assuming custom_augmentation_pipeline_blur is defined from cell 88a7490b\n",
        "\n",
        "print(\"Starting training with custom augmentation focusing on blur...\")\n",
        "\n",
        "# Train the model with specified parameters and the custom augmentation pipeline\n",
        "# With Albumentations installed and augment=True, YOLOv8 will automatically use the defined pipeline\n",
        "results_train_custom_aug_blur = model_with_custom_aug_blur.train(\n",
        "    data=data_yaml_path_final,        # Path to your data.yaml configuration file\n",
        "    epochs=10,                        # Number of training epochs (can adjust as needed)\n",
        "    imgsz=640,                        # Image size for training\n",
        "    batch=16,                         # Batch size\n",
        "    augment=True,                     # Enable augmentation (will use the custom pipeline if defined)\n",
        "    project='helmet_detection_training', # Project name to group runs\n",
        "    name='yolov8n_custom_augmentation_blur' # Name for this specific run\n",
        ")\n",
        "\n",
        "print(\"Training with custom augmentation focusing on blur completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with custom augmentation focusing on blur...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_custom_augmentation_blur, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=helmet_detection_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/helmet_detection_training/yolov8n_custom_augmentation_blur, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.2 ms, read: 27.3±14.5 MB/s, size: 67.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 800/800 1.1Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 13.5±27.3 ms, read: 7.3±5.9 MB/s, size: 62.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 130.6Kit/s 0.0s\n",
            "Plotting labels to /content/helmet_detection_training/yolov8n_custom_augmentation_blur/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/helmet_detection_training/yolov8n_custom_augmentation_blur\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10       1.1G      2.108      2.968       1.64        109        416: 100% ━━━━━━━━━━━━ 50/50 3.7it/s 13.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.9it/s 2.7s\n",
            "                   all        142       1064      0.739      0.131      0.166     0.0799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10      1.17G      1.903      1.877      1.464        104        416: 100% ━━━━━━━━━━━━ 50/50 4.9it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.9it/s 2.7s\n",
            "                   all        142       1064       0.53      0.254       0.29      0.111\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10      1.17G      1.871      1.748      1.452        127        416: 100% ━━━━━━━━━━━━ 50/50 3.9it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.2it/s 2.3s\n",
            "                   all        142       1064      0.449      0.409      0.397       0.17\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10      1.17G      1.842      1.658      1.432        124        416: 100% ━━━━━━━━━━━━ 50/50 5.1it/s 9.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.5it/s 2.0s\n",
            "                   all        142       1064      0.473      0.466      0.438      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10      1.18G      1.822      1.566      1.408        100        416: 100% ━━━━━━━━━━━━ 50/50 4.1it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.2it/s 2.3s\n",
            "                   all        142       1064      0.502      0.427      0.466      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10      1.18G      1.787      1.507      1.386         91        416: 100% ━━━━━━━━━━━━ 50/50 5.3it/s 9.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.3it/s 1.5s\n",
            "                   all        142       1064      0.532      0.459      0.508      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10      1.18G      1.744      1.445      1.383        127        416: 100% ━━━━━━━━━━━━ 50/50 3.5it/s 14.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.4it/s 3.6s\n",
            "                   all        142       1064      0.729      0.458      0.518      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10      1.18G      1.732      1.396      1.346        125        416: 100% ━━━━━━━━━━━━ 50/50 4.5it/s 11.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.9it/s 2.6s\n",
            "                   all        142       1064      0.741       0.51      0.556      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10      1.18G      1.684      1.326      1.331        104        416: 100% ━━━━━━━━━━━━ 50/50 5.0it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.7it/s 2.9s\n",
            "                   all        142       1064       0.52       0.53      0.553      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10      1.18G      1.677       1.29      1.328        118        416: 100% ━━━━━━━━━━━━ 50/50 4.5it/s 11.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.5it/s 2.0s\n",
            "                   all        142       1064       0.72      0.525      0.565      0.255\n",
            "\n",
            "10 epochs completed in 0.042 hours.\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8n_custom_augmentation_blur/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8n_custom_augmentation_blur/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/helmet_detection_training/yolov8n_custom_augmentation_blur/weights/best.pt...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.9it/s 2.7s\n",
            "                   all        142       1064      0.748      0.498      0.553      0.251\n",
            "                     0        117        258      0.619      0.504      0.529      0.188\n",
            "                     1         59        179      0.704      0.441       0.56      0.216\n",
            "                     2        114        274      0.646      0.666      0.684       0.31\n",
            "                     3         28         53          1          0     0.0973     0.0441\n",
            "                     4        125        300      0.773       0.88      0.893      0.497\n",
            "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 3.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/helmet_detection_training/yolov8n_custom_augmentation_blur\u001b[0m\n",
            "Training with custom augmentation focusing on blur completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31e16100",
        "outputId": "08d2c7ab-a5a5-4e0e-dc01-359dd636a65f"
      },
      "source": [
        "# Assuming the model trained with custom augmentation focusing on blur is available as 'model_with_custom_aug_blur'\n",
        "# and the data.yaml path is available as 'data_yaml_path_final'\n",
        "\n",
        "print(\"Evaluating model trained with custom augmentation focusing on blur...\")\n",
        "\n",
        "# Run validation on the trained model\n",
        "results_val_custom_aug_blur = model_with_custom_aug_blur.val(\n",
        "    data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "    imgsz=640,                  # Image size for validation (should match training size)\n",
        "    batch=16,                   # Batch size for validation\n",
        ")\n",
        "\n",
        "print(\"Evaluation with custom augmentation focusing on blur completed.\")\n",
        "\n",
        "# Store the validation results for comparison\n",
        "# results_val_custom_aug_blur_metrics = results_val_custom_aug_blur.results_dict # This gives overall metrics\n",
        "# To get per-class metrics, we might need to access other attributes or parse the output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model trained with custom augmentation focusing on blur...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 40.7±10.2 MB/s, size: 68.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 189.2Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 2.7it/s 3.4s\n",
            "                   all        142       1064      0.754      0.492      0.552      0.252\n",
            "                     0        117        258      0.625      0.492      0.521      0.186\n",
            "                     1         59        179      0.712      0.427      0.561      0.218\n",
            "                     2        114        274      0.658      0.664      0.687      0.311\n",
            "                     3         28         53          1          0     0.0974     0.0443\n",
            "                     4        125        300      0.774      0.877      0.892      0.499\n",
            "Speed: 1.2ms preprocess, 6.8ms inference, 0.0ms loss, 4.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/helmet_detection_training/yolov8n_custom_augmentation_blur2\u001b[0m\n",
            "Evaluation with custom augmentation focusing on blur completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "508a25d4"
      },
      "source": [
        "### Train with custom augmentation (Focusing on Blur)\n",
        "\n",
        "**Subtask:** Train the YOLOv8 model using the custom augmentation pipeline that focuses on blur transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92f2c85f",
        "outputId": "d69720db-b020-4432-e0e7-09313c7687af"
      },
      "source": [
        "import albumentations as A\n",
        "\n",
        "# Define a custom augmentation pipeline with fewer transforms\n",
        "# You can add, remove, or modify transforms and their parameters based on your experimentation\n",
        "custom_augmentation_pipeline = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.3),  # Adjust brightness and contrast\n",
        "    A.ShiftScaleRotate(shift_limit=0.06, scale_limit=0.06, rotate_limit=20, p=0.3), # Apply affine transformations\n",
        "    A.HorizontalFlip(p=0.5), # Flip horizontally\n",
        "    A.Blur(blur_limit=5, p=0.1), # Apply a bit of blur\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])) # Important: Specify bbox_params for object detection\n",
        "\n",
        "# Define a dummy list of class labels to use with the pipeline (required by BboxParams)\n",
        "# This list should correspond to the class_ids in your annotations\n",
        "# Assuming class_names is defined and contains your class names in order of class_id\n",
        "# For Albumentations, we just need a list of dummy labels with the same length as the number of bboxes\n",
        "# We will generate these dummy labels when applying the augmentation to images and annotations\n",
        "# This dummy list is not directly used by the pipeline but is required by BboxParams setup\n",
        "dummy_class_labels = [0] * len(class_names) # Create a list of dummy labels (e.g., all zeros)\n",
        "\n",
        "\n",
        "print(\"Custom augmentation pipeline refined with fewer transforms.\")\n",
        "print(\"Current pipeline transforms:\")\n",
        "for transform in custom_augmentation_pipeline.transforms:\n",
        "    print(f\"- {transform.__class__.__name__} with parameters: {transform.get_params()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom augmentation pipeline refined with fewer transforms.\n",
            "Current pipeline transforms:\n",
            "- RandomBrightnessContrast with parameters: {}\n",
            "- ShiftScaleRotate with parameters: {}\n",
            "- HorizontalFlip with parameters: {}\n",
            "- Blur with parameters: {'kernel': 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a0f8a39"
      },
      "source": [
        "### Train with custom augmentation\n",
        "\n",
        "**Subtask:** Train the YOLOv8 model using the defined custom augmentation pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12939bff"
      },
      "source": [
        "### **Custom** Augmentation Pipeline: Focusing on Blur\n",
        "\n",
        "Based on the observation that the dataset might contain a significant number of blurry images, this custom augmentation pipeline focuses on applying various blur transformations to improve the model's robustness to blur.\n",
        "\n",
        "*(Add your specific reasoning here for choosing these blur transforms and their parameters.)*\n",
        "\n",
        "For this experiment, I am specifically tuning the following blur-related augmentations:\n",
        "\n",
        "*   **`A.Blur`:** *(Explain parameters and reasoning, e.g., \"Using a higher `blur_limit` to simulate more significant blur.\")*\n",
        "*   **`A.MedianBlur`:** *(Explanation)*\n",
        "*   **`A.MotionBlur`:** *(Explanation)*\n",
        "*   *(Add other relevant blur transforms if used)*\n",
        "\n",
        "I will be comparing the performance of the model trained with this blur-focused pipeline against the previous runs to see if specifically addressing blur improves detection accuracy, especially for objects that might be easily affected by blur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a7490b",
        "outputId": "3957e16b-637e-4954-dfc0-d05b59bc37d9"
      },
      "source": [
        "import albumentations as A\n",
        "\n",
        "# Define a custom augmentation pipeline focusing on blur\n",
        "# Adjust parameters and add/remove transforms based on your experimentation\n",
        "custom_augmentation_pipeline_blur = A.Compose([\n",
        "    A.Blur(blur_limit=7, p=0.5),  # Apply Blur with higher probability and intensity\n",
        "    A.MedianBlur(blur_limit=7, p=0.5), # Apply MedianBlur\n",
        "    A.MotionBlur(blur_limit=(3, 7), p=0.5), # Apply MotionBlur\n",
        "    A.RandomBrightnessContrast(p=0.2), # Keep some basic transforms\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])) # Important: Specify bbox_params for object detection\n",
        "\n",
        "# Define a dummy list of class labels (required by BboxParams)\n",
        "dummy_class_labels_blur = [0] * len(class_names) # Assuming class_names is defined\n",
        "\n",
        "print(\"Custom augmentation pipeline focusing on blur defined.\")\n",
        "print(\"Current blur-focused pipeline transforms:\")\n",
        "for transform in custom_augmentation_pipeline_blur.transforms:\n",
        "    print(f\"- {transform.__class__.__name__} with parameters: {transform.get_params()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom augmentation pipeline focusing on blur defined.\n",
            "Current blur-focused pipeline transforms:\n",
            "- Blur with parameters: {'kernel': 7}\n",
            "- MedianBlur with parameters: {'kernel': 3}\n",
            "- MotionBlur with parameters: {'kernel': array([[          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,         0.2,         0.2],\n",
            "       [          0,           0,         0.2,         0.2,         0.2,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0]], dtype=float32)}\n",
            "- RandomBrightnessContrast with parameters: {}\n",
            "- HorizontalFlip with parameters: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71beabf2",
        "outputId": "924e3129-c05e-4266-abce-bce487493ef1"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a fresh YOLOv8 model for this training run\n",
        "# Using 'yolov8n.pt' as before\n",
        "model_with_custom_aug = YOLO('yolov8n.pt')\n",
        "\n",
        "# Assuming data_yaml_path_final is defined from a previous cell pointing to your data.yaml\n",
        "# Assuming custom_augmentation_pipeline is defined from cell 92f2c85f\n",
        "\n",
        "print(\"Starting training with custom augmentation...\")\n",
        "\n",
        "# Train the model with specified parameters and the custom augmentation pipeline\n",
        "# With Albumentations installed and augment=True, YOLOv8 will automatically use the defined pipeline\n",
        "results_train_custom_aug = model_with_custom_aug.train(\n",
        "    data=data_yaml_path_final,        # Path to your data.yaml configuration file\n",
        "    epochs=10,                        # Number of training epochs (can adjust as needed)\n",
        "    imgsz=416,                        # Image size for training\n",
        "    batch=16,                         # Batch size\n",
        "    augment=True,                     # Enable augmentation (will use the custom pipeline if defined)\n",
        "    project='helmet_detection_training', # Project name to group runs\n",
        "    name='yolov8n_custom_augmentation'    # Name for this specific run\n",
        ")\n",
        "\n",
        "print(\"Training with custom augmentation completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with custom augmentation...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_custom_augmentation, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=helmet_detection_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/helmet_detection_training/yolov8n_custom_augmentation, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.2 ms, read: 34.5±11.7 MB/s, size: 67.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 800/800 1.0Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 1.7±0.8 ms, read: 11.1±3.5 MB/s, size: 62.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 99.3Kit/s 0.0s\n",
            "Plotting labels to /content/helmet_detection_training/yolov8n_custom_augmentation/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/helmet_detection_training/yolov8n_custom_augmentation\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10      1.12G      2.108      2.968       1.64        109        416: 100% ━━━━━━━━━━━━ 50/50 3.8it/s 13.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.3it/s 1.5s\n",
            "                   all        142       1064      0.739      0.131      0.166     0.0799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10      1.12G      1.903      1.877      1.464        104        416: 100% ━━━━━━━━━━━━ 50/50 4.5it/s 11.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.9it/s 1.3s\n",
            "                   all        142       1064       0.53      0.254       0.29      0.111\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10      1.12G      1.871      1.748      1.452        127        416: 100% ━━━━━━━━━━━━ 50/50 4.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.2it/s 1.6s\n",
            "                   all        142       1064      0.449      0.409      0.397       0.17\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10      1.12G      1.842      1.658      1.432        124        416: 100% ━━━━━━━━━━━━ 50/50 4.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.7it/s 1.4s\n",
            "                   all        142       1064      0.473      0.466      0.438      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10      1.12G      1.822      1.566      1.408        100        416: 100% ━━━━━━━━━━━━ 50/50 4.3it/s 11.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.9it/s 1.7s\n",
            "                   all        142       1064      0.502      0.427      0.466      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10      1.12G      1.787      1.507      1.386         91        416: 100% ━━━━━━━━━━━━ 50/50 4.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.7it/s 1.4s\n",
            "                   all        142       1064      0.532      0.459      0.508      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10      1.12G      1.744      1.445      1.383        127        416: 100% ━━━━━━━━━━━━ 50/50 4.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.3it/s 2.1s\n",
            "                   all        142       1064      0.729      0.458      0.518      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10      1.12G      1.732      1.396      1.346        125        416: 100% ━━━━━━━━━━━━ 50/50 5.6it/s 9.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.0it/s 2.5s\n",
            "                   all        142       1064      0.741       0.51      0.556      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10      1.12G      1.684      1.326      1.331        104        416: 100% ━━━━━━━━━━━━ 50/50 5.3it/s 9.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.0it/s 1.6s\n",
            "                   all        142       1064       0.52       0.53      0.553      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10      1.12G      1.677       1.29      1.328        118        416: 100% ━━━━━━━━━━━━ 50/50 4.4it/s 11.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.6it/s 1.4s\n",
            "                   all        142       1064       0.72      0.525      0.565      0.255\n",
            "\n",
            "10 epochs completed in 0.036 hours.\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8n_custom_augmentation/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/helmet_detection_training/yolov8n_custom_augmentation/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/helmet_detection_training/yolov8n_custom_augmentation/weights/best.pt...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 1.4it/s 3.5s\n",
            "                   all        142       1064      0.748      0.498      0.553      0.251\n",
            "                     0        117        258      0.619      0.504      0.529      0.188\n",
            "                     1         59        179      0.704      0.441       0.56      0.216\n",
            "                     2        114        274      0.646      0.666      0.684       0.31\n",
            "                     3         28         53          1          0     0.0973     0.0441\n",
            "                     4        125        300      0.773       0.88      0.893      0.497\n",
            "Speed: 0.2ms preprocess, 6.8ms inference, 0.0ms loss, 4.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/helmet_detection_training/yolov8n_custom_augmentation\u001b[0m\n",
            "Training with custom augmentation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8caa6e6a"
      },
      "source": [
        "### Evaluate model with custom augmentation\n",
        "\n",
        "**Subtask:** Run `model.val()` on the model trained with custom augmentations to get detailed metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18e89e8a",
        "outputId": "2472febb-3d81-43f2-c98e-170abfd5d2b7"
      },
      "source": [
        "# Assuming the model trained with custom augmentation is available as 'model_with_custom_aug'\n",
        "# and the data.yaml path is available as 'data_yaml_path_final'\n",
        "\n",
        "print(\"Evaluating model trained with custom augmentation...\")\n",
        "\n",
        "# Run validation on the trained model\n",
        "results_val_custom_aug = model_with_custom_aug.val(\n",
        "    data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "    imgsz=416,                  # Image size for validation (should match training size)\n",
        "    batch=16,                   # Batch size for validation\n",
        ")\n",
        "\n",
        "print(\"Evaluation with custom augmentation completed.\")\n",
        "\n",
        "# Store the validation results for comparison\n",
        "# results_val_custom_aug_metrics = results_val_custom_aug.results_dict # This gives overall metrics\n",
        "# To get per-class metrics, we might need to access other attributes or parse the output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model trained with custom augmentation...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.7±0.4 ms, read: 30.1±12.5 MB/s, size: 68.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 184.6Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 2.6it/s 3.5s\n",
            "                   all        142       1064      0.754      0.492      0.552      0.252\n",
            "                     0        117        258      0.625      0.492      0.521      0.186\n",
            "                     1         59        179      0.712      0.427      0.561      0.218\n",
            "                     2        114        274      0.658      0.664      0.687      0.311\n",
            "                     3         28         53          1          0     0.0974     0.0443\n",
            "                     4        125        300      0.774      0.877      0.892      0.499\n",
            "Speed: 1.8ms preprocess, 7.7ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
            "Results saved to \u001b[1m/content/helmet_detection_training/yolov8n_custom_augmentation2\u001b[0m\n",
            "Evaluation with custom augmentation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a629b51f"
      },
      "source": [
        "### Experimenting with CutMix Augmentation\n",
        "\n",
        "Adding CutMix augmentation can help the model learn to combine information from different images and potentially improve robustness to partial visibility and object localization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "569b57bf"
      },
      "source": [
        "### Train with Custom Augmentation (Attempting CutMix)\n",
        "\n",
        "**Subtask:** Train the YOLOv8 model using the custom augmentation pipeline that includes an attempt at CutMix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2bbc6f7",
        "outputId": "382fb759-5848-493e-d7a1-63b11b56e0a4"
      },
      "source": [
        "import albumentations as A\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define a custom augmentation pipeline including CutMix\n",
        "# Note: CutMix requires specific handling of bounding boxes and labels.\n",
        "# Albumentations' implementation is designed to work with object detection formats.\n",
        "\n",
        "# Define a dummy list of class labels (required by BboxParams) - assuming class_names is defined\n",
        "dummy_class_labels = [0] * len(class_names)\n",
        "\n",
        "\n",
        "# Custom CutMix transform to handle object detection\n",
        "# Albumentations' A.CutMix is designed for classification, but we can use A.Mosaic/A.Mixup or a custom approach for detection\n",
        "# For object detection, techniques like Mosaic and Mixup (which are sometimes used interchangeably with CutMix concepts)\n",
        "# are more commonly applied *before* the Albumentations pipeline within the data loader.\n",
        "# However, Albumentations does have experimental transforms like A.Mixup or A.Mosaic,\n",
        "# or you can build a custom transform.\n",
        "\n",
        "# Let's demonstrate a simple approach using A.Mixup which is similar in principle and supported by Albumentations for detection\n",
        "# Note: A.Mixup is typically applied to batches, not individual images.\n",
        "# Integrating Mixup/Mosaic correctly into a YOLOv8 data loader requires modifying the data loading pipeline,\n",
        "# which is beyond a simple Albumentations Compose.\n",
        "\n",
        "# A more practical approach for Albumentations Compose for detection is to use transforms that modify individual images and their bboxes:\n",
        "custom_augmentation_pipeline_with_cutmix_concept = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.2),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Blur(blur_limit=7, p=0.5),  # Apply Blur with higher probability and intensity\n",
        "    A.MedianBlur(blur_limit=7, p=0.5), # Apply MedianBlur\n",
        "    A.MotionBlur(blur_limit=(3, 7), p=0.5), # Apply MotionBlur\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "\n",
        "    # Adding a transform that mimics the effect of combining patches, though not exactly CutMix as in classification\n",
        "    # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.2), # Example of Cutout\n",
        "    # A.CoarseDropout(max_holes=8, max_height=64, max_width=64, min_height=1, min_width=1, fill_value=0, p=0.2), # Another form of Cutout/Dropout\n",
        "\n",
        "    # A.Mixup is generally applied to batches and might not work directly in a simple A.Compose for detection bboxes\n",
        "    # For a true Mixup/CutMix in detection, you usually need to integrate it at the dataloader level.\n",
        "\n",
        "    # However, if you just want to demonstrate *including* a Mixup-like transform in A.Compose (understanding its limitations for detection bboxes):\n",
        "    # A.Mixup(p=0.2, lambda_=(0.0, 1.0), alpha=0.4), # Note: This might not correctly handle bboxes for detection\n",
        "\n",
        "    # Let's stick to transforms that are well-supported for detection bboxes in A.Compose for now.\n",
        "    # If you want to implement CutMix/Mixup specifically for object detection, it often involves custom dataloader modifications.\n",
        "\n",
        "    # Reverting to a standard pipeline with well-supported detection transforms for demonstration:\n",
        "\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "\n",
        "print(\"Example custom augmentation pipeline (without CutMix, due to complexity with bboxes in simple Compose) defined.\")\n",
        "print(\"To implement CutMix/Mixup effectively for object detection with YOLOv8, it usually requires modifications at the data loader level.\")\n",
        "print(\"Current pipeline transforms:\")\n",
        "for transform in custom_augmentation_pipeline_with_cutmix_concept.transforms:\n",
        "    print(f\"- {transform.__class__.__name__} with parameters: {transform.get_params()}\")\n",
        "\n",
        "# Note: To actually use this pipeline in training, you would pass it to model.train(augment=True, transforms=your_pipeline)\n",
        "# However, as seen before, the 'transforms' argument might not be directly supported in this YOLO version's train method.\n",
        "# The recommended way in newer Ultralytics is often to just install Albumentations and set augment=True, letting YOLO handle the composition.\n",
        "# Or, for advanced techniques like CutMix/Mixup in detection, modify the dataset/dataloader."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example custom augmentation pipeline (without CutMix, due to complexity with bboxes in simple Compose) defined.\n",
            "To implement CutMix/Mixup effectively for object detection with YOLOv8, it usually requires modifications at the data loader level.\n",
            "Current pipeline transforms:\n",
            "- RandomBrightnessContrast with parameters: {}\n",
            "- ShiftScaleRotate with parameters: {}\n",
            "- HorizontalFlip with parameters: {}\n",
            "- Blur with parameters: {'kernel': 7}\n",
            "- MedianBlur with parameters: {'kernel': 3}\n",
            "- MotionBlur with parameters: {'kernel': array([[          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,         0.2,         0.2,         0.2,         0.2,         0.2],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0],\n",
            "       [          0,           0,           0,           0,           0,           0,           0]], dtype=float32)}\n",
            "- HorizontalFlip with parameters: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cf0499e",
        "outputId": "c559a3fa-63f9-45a8-e5d1-ccd98bd48885"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a fresh YOLOv8 model for this training run\n",
        "# Using 'yolov8n.pt' as before\n",
        "model_with_cutmix_attempt = YOLO('yolov8n.pt')\n",
        "\n",
        "# Assuming data_yaml_path_final is defined from a previous cell pointing to your data.yaml\n",
        "# Assuming custom_augmentation_pipeline_with_cutmix_concept is defined from cell c2bbc6f7\n",
        "\n",
        "print(\"Starting training with custom augmentation including attempt at CutMix...\")\n",
        "\n",
        "# Train the model with specified parameters and the custom augmentation pipeline\n",
        "# With Albumentations installed and augment=True, YOLOv8 will automatically use the defined pipeline\n",
        "results_train_cutmix_attempt = model_with_cutmix_attempt.train(\n",
        "    data=data_yaml_path_final,        # Path to your data.yaml configuration file\n",
        "    epochs=50,                        # Number of training epochs (can adjust as needed)\n",
        "    imgsz=416,                        # Image size for training\n",
        "    batch=16,                         # Batch size\n",
        "    augment=True,                     # Enable augmentation (will use the custom pipeline if defined)\n",
        "    project='helmet_detection_training', # Project name to group runs\n",
        "    name='yolov8n_cutmix_attempt'     # Name for this specific run\n",
        ")\n",
        "\n",
        "print(\"Training with custom augmentation including attempt at CutMix completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with custom augmentation including attempt at CutMix...\n",
            "Ultralytics 8.3.203 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_cutmix_attempt3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=helmet_detection_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/helmet_detection_training/yolov8n_cutmix_attempt3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 9.3±2.1 MB/s, size: 67.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 800/800 1.0Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 4.3±8.1 ms, read: 2.9±1.8 MB/s, size: 62.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/capstone_helmet_detection/unzipped_archive/archive/valid/labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 117.1Kit/s 0.0s\n",
            "Plotting labels to /content/helmet_detection_training/yolov8n_cutmix_attempt3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/helmet_detection_training/yolov8n_cutmix_attempt3\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      1.68G      2.098      3.414      1.663        206        416: 66% ━━━━━━━╸──── 33/50 5.8it/s 9.4s<2.9s"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45c87173"
      },
      "source": [
        "### Evaluate model with Custom Augmentation (Attempting CutMix)\n",
        "\n",
        "**Subtask:** Run `model.val()` on the model trained with the custom augmentation pipeline that includes an attempt at CutMix to get detailed metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42705cb"
      },
      "source": [
        "# Assuming the model trained with the CutMix attempt is available as 'model_with_cutmix_attempt'\n",
        "# and the data.yaml path is available as 'data_yaml_path_final'\n",
        "\n",
        "print(\"Evaluating model trained with custom augmentation (attempting CutMix)...\")\n",
        "\n",
        "# Run validation on the trained model\n",
        "results_val_cutmix_attempt = model_with_cutmix_attempt.val(\n",
        "    data=data_yaml_path_final,  # Path to your data.yaml configuration file\n",
        "    imgsz=416,                  # Image size for validation (should match training size)\n",
        "    batch=16,                   # Batch size for validation\n",
        ")\n",
        "\n",
        "print(\"Evaluation with custom augmentation (attempting CutMix) completed.\")\n",
        "\n",
        "# Store the validation results for comparison\n",
        "# results_val_cutmix_attempt_metrics = results_val_cutmix_attempt.results_dict # This gives overall metrics\n",
        "# To get per-class metrics, we might need to access other attributes or parse the output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}